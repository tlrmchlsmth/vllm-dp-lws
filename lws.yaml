apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
    name: __LWS_NAME__
spec:
    replicas: 1
    leaderWorkerTemplate:
        size: 4
        restartPolicy: RecreateGroupOnPodRestart

        workerTemplate:
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                      - matchExpressions:
                          - key: gpu.nvidia.com/model
                            operator: In
                            values:
                              - H200

              containers:
              - name: vllm-worker
                image: "quay.io/tms/vllm-dev-deepep:0.1.0"
                imagePullPolicy: Always
                workingDir: /code
                stdin: true
                tty: true
                command: ["/bin/sh","-c"]
                args:
                  - |
                    #################
                    # Interactive section
                    #################
                    # export VENV_PATH="/code/venv"
                    # source /init-scripts/common.sh
                    # /init-scripts/dotfiles.sh
                    # sleep infinity
                    #################
                    #
                    #################
                    # Install vLLM
                    #################
                    VLLM_USE_PRECOMPILED=1 /init-scripts/vllm.sh
                    #################
                    # RUN vLLM
                    #################
                    START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
                    if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
                      #################
                      # Leader-only launch
                      #################
                      exec /app/venv/bin/vllm serve \
                        __MODEL__ \
                        --port 8080 \
                        --disable-log-requests \
                        --enable-expert-parallel \
                        --tensor-parallel-size $TP_SIZE \
                        --data-parallel-size $DP_SIZE \
                        --data-parallel-size-local $DP_SIZE_LOCAL \
                        --data-parallel-address $(LWS_LEADER_ADDRESS) \
                        --data-parallel-rpc-port 5555 \
                        --data-parallel-start-rank $START_RANK \
                        --trust-remote-code
                    else
                      #################
                      # Worker-only launch
                      #################
                      exec /app/venv/bin/vllm serve \
                        __MODEL__ \
                        --port 8080 \
                        --disable-log-requests \
                        --enable-expert-parallel \
                        --tensor-parallel-size $TP_SIZE \
                        --data-parallel-size $DP_SIZE \
                        --data-parallel-size-local $DP_SIZE_LOCAL \
                        --data-parallel-address $(LWS_LEADER_ADDRESS) \
                        --data-parallel-rpc-port 5555 \
                        --data-parallel-start-rank $START_RANK \
                        --trust-remote-code \
                        --headless
                    fi
##################
# Paste these args into the above vllm serve commands
# to enable EPLB and API server scaleout.
##################
#  --enable-eplb \
#  --num-redundant-experts 32 \
#  --eplb-window-size 1000 \
#  --eplb-step-interval 3000 \
##################
#  --api-server-count=8 \
##################
                env:
                  - name: VLLM_TORCH_PROFILER_DIR
                    value: "/code/traces"
                  - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
                    value: "1"
                  - name: DP_SIZE
                    value: "32"
                  - name: TP_SIZE
                    value: "1"
                  - name: DP_SIZE_LOCAL
                    value: "8"
                  - name: VLLM_REPO_URL
                    value: "https://github.com/vllm-project/vllm.git"
                  - name: VLLM_BRANCH
                    value: "main"
                  - name: VLLM_USE_DEEP_GEMM
                    value: "1"
                  - name: VLLM_ALL2ALL_BACKEND
                  #  value: "naive"
                  #  value: "pplx"
                  #  value: "deepep_high_throughput"
                    value: "deepep_low_latency"
                  # Needed for GDRCOPY to be used.
                  # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
                  - name: NVIDIA_GDRCOPY
                    value: "enabled"
                  - name: NVSHMEM_DEBUG
                    value: "INFO"
                  # Uncomment for debugging
                  #- name: NVSHMEM_DEBUG_SUBSYS
                  #  value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
                  - name: NVSHMEM_REMOTE_TRANSPORT
                    value: "ibgda"
                  - name: NVSHMEM_IB_ENABLE_IBGDA
                    value: "true"
                  - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                    value: "eth0"
                  - name: GLOO_SOCKET_IFNAME
                    value: "eth0"
                  - name: NCCL_SOCKET_IFNAME
                    value: "eth0"
                  #- name: NCCL_DEBUG
                  #  value: "info"
                  - name: NCCL_IB_HCA
                    value: "ibp"
                  - name: VLLM_LOGGING_LEVEL
                    value: "DEBUG"
                  - name: HF_HUB_CACHE
                    value: /huggingface-cache
                  - name: HF_TOKEN
                    valueFrom:
                      secretKeyRef:
                        name: hf-secret
                        key: HF_TOKEN
                        optional: true
                  - name: GH_TOKEN_FROM_SECRET
                    valueFrom:
                      secretKeyRef:
                        name: gh-token-secret
                        key: GH_TOKEN
                        optional: true
                  - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                    value: "6555"
                  - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                    valueFrom:
                      fieldRef:
                        fieldPath: status.podIP

                securityContext:
                  capabilities:
                    add: 
                    - "IPC_LOCK"
                    - "SYS_RAWIO"
                resources:
                  limits:
                    memory: 512Gi
                    ephemeral-storage: 64Gi
                    nvidia.com/gpu: "8"
                    rdma/ib: 1
                  requests:
                    cpu: 32
                    memory: 512Gi
                    ephemeral-storage: 64Gi 
                    nvidia.com/gpu: "8"
                    rdma/ib: 1
                volumeMounts:
                  - name: dshm
                    mountPath: /dev/shm
                  - name: init-scripts-volume
                    mountPath: /init-scripts
                  - name: hf-cache
                    mountPath: /huggingface-cache
                  - name: vllm
                    mountPath: /code
              volumes:
                # Volume for the init script from ConfigMap
                - name: init-scripts-volume
                  configMap:
                    name: vllm-init-scripts-config
                    defaultMode: 0755 # Set execute permissions for the script
                # Needed for NCCL to function
                - name: dshm
                  emptyDir:
                    medium: Memory
                    sizeLimit: 1Gi
                - name: hf-cache
                  persistentVolumeClaim:
                    claimName: tms-hf-cache
                - name: vllm
                  persistentVolumeClaim:
                    claimName: tms-vllm

---
apiVersion: v1
kind: Service
metadata:
    name: __SERVICE_NAME__
spec:
    ports:
        - name: http
          port: 8080
          protocol: TCP
          targetPort: 8080
    selector:
        leaderworkerset.sigs.k8s.io/name: __LWS_NAME__
        leaderworkerset.sigs.k8s.io/worker-index: "0"
    type: ClusterIP
